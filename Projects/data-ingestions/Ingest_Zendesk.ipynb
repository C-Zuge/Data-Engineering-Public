{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c7101e-b2a0-496b-af5f-3d50031175cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Utils/Functions/core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de88131f-1807-46c8-86a6-b15379333661",
     "showTitle": true,
     "title": "Importa as bibliotecas"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import concurrent.futures\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from time import sleep, mktime\n",
    "from uuid import uuid4\n",
    "import json\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4bdd718-8f05-4fb8-a952-9deb2f2e69b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Cria as variáveis de LOG\n",
    "format_log = '%Y-%m-%d %H:%M:%S'\n",
    "dtInicio = datetime.today() - timedelta(hours=3)\n",
    "dtInicio_format = dtInicio.strftime(format_log)\n",
    "tipo_log = 'API'\n",
    "camada = 'landing'\n",
    "emissor = '<org>'\n",
    "atividade = '<activity_desc>'\n",
    "origem = 'RestService'\n",
    "destino = 'AzureBlobFS'\n",
    "execUrl = ' '\n",
    "\n",
    "try:\n",
    "    infos = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson()) # captura as informações do job que executa o notebook\n",
    "    orgId = infos['tags']['orgId']\n",
    "    runId = infos['tags']['multitaskParentRunId']\n",
    "    jobId = infos['tags']['jobId']\n",
    "    if orgId == '2960871991268730': # Monta a URL caso seja o ID do ambiente de DEV\n",
    "        execUrl = f'https://adb-{orgId}.10.azuredatabricks.net/?o={orgId}#job/{jobId}/run/{runId}' # cria a url de execução do \n",
    "    else: # Monta a URL caso seja o ID do ambiente de PROD\n",
    "        execUrl = f'https://adb-{orgId}.15.azuredatabricks.net/?o={orgId}#job/{jobId}/run/{runId}' # cria a url de execução do \n",
    "except:\n",
    "    print('Campo de URL não pode ser identificado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1779b154-57d3-44f8-99b3-8c6e3eb93dd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog prod;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa697c2d-d59a-41cb-90c6-d589f5c10e55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"dt_ingestao\", \"\")\n",
    "\n",
    "dt_ingestao = getArgument(\"dt_ingestao\").upper().strip()\n",
    "\n",
    "location_landing = spark.sql(\"show external locations\").select(\"url\").where(\"name = 'landing-area'\").collect()[0][0]\n",
    "location_flat_file = spark.sql(\"show external locations\").select(\"url\").where(\"name = 'flatfile-area'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2ccdaeb-9495-4512-be4b-6cf6f3022fbe",
     "showTitle": true,
     "title": "Declara variáveis para cargas incrementais"
    }
   },
   "outputs": [],
   "source": [
    "# Formata o dt_ingestao\n",
    "format_timestamp = '%Y-%m-%d %H:%M:%S.%f'\n",
    "dt_ingestao = datetime.now() if dt_ingestao == \"\" else datetime.strptime(dt_ingestao, format_timestamp)\n",
    "\n",
    "# Pega o horário atual e tira 3 horas para converter para BRT (UTC -3)\n",
    "# Tira 5 minutos por conta de um limite da API\n",
    "dt_fim = dt_ingestao - timedelta(hours=3) - timedelta(minutes=5)\n",
    "dt_inicio = dt_fim - timedelta(days=1)\n",
    "\n",
    "# Força as horas, minutos e segundos em 0\n",
    "dt_inicio = datetime(dt_inicio.year, dt_inicio.month, dt_inicio.day, 0, 0, 0)\n",
    "\n",
    "# Separa as datas no formato UNIX\n",
    "dt_fim_unix = int(mktime(dt_fim.timetuple()))\n",
    "dt_inicio_unix = int(mktime(dt_inicio.timetuple()))\n",
    "\n",
    "# dt_inicio_unix = 1546300800\n",
    "\n",
    "# Transforma as datas em STRING utilizando um formato de timestamp\n",
    "dt_fim = dt_fim.strftime(format_timestamp)\n",
    "dt_inicio = dt_inicio.strftime(format_timestamp)\n",
    "\n",
    "print(f\"dt_inicio: {dt_inicio} | unix: {dt_inicio_unix}\")\n",
    "print(f\"dt_fim   : {dt_fim} | unix: {dt_fim_unix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef391cea-e754-4e0c-93e4-26c498c5b7c7",
     "showTitle": true,
     "title": "Declara as variáveis de conexão da API"
    }
   },
   "outputs": [],
   "source": [
    "username = dbutils.secrets.get(\"scope-vault-data\", \"zendesk-user\")\n",
    "password = dbutils.secrets.get(\"scope-vault-data\", \"zendesk-pass\")\n",
    "\n",
    "domain = \"zendesk\"\n",
    "subdomain = \"<subdomain>\"\n",
    "threads = 10\n",
    "baseUrl = f\"https://{subdomain}.{domain}.com\"\n",
    "\n",
    "dir_flatfile = f'{location_flat_file}/<org>/zendesk'\n",
    "dir_flatfile_ro = f'/dbfs/FileSotre/flat-files/.../<org>/zendesk'\n",
    "\n",
    "org_memberships_path = dir_flatfile+\"/organization_memberships/last-page-org-memberships.txt\"\n",
    "org_memberships_path_ro = dir_flatfile_ro+\"/organization_memberships/last-page-org-memberships.txt\"\n",
    "\n",
    "print(baseUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffa5ac26-4a91-4f28-8c87-5acdf73c0073",
     "showTitle": true,
     "title": "Tabelas cadastradas para carga"
    }
   },
   "outputs": [],
   "source": [
    "# Pega apenas cargas que possuem valor preenchido no campo \"ds_Url\" para não trazer tabelas que são geradas a partir de outro evento na landing.\n",
    "df_param = fn_ConsultaJdbc(\"\"\"\n",
    "    SELECT pca.*\n",
    "    FROM ctl.ADF_Parametro_Carga_API pca\n",
    "    WHERE pca.fl_ativo = 1\n",
    "    and nm_Sistema = 'zendesk'\n",
    "    and ds_Url is not null\n",
    "\"\"\")\n",
    "\n",
    "data_param = df_param.collect()\n",
    "\n",
    "display(df_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e61e4c0-daf8-49f4-be2f-6c221a027d4b",
     "showTitle": true,
     "title": "Funções utilizadas no processo de Carga"
    }
   },
   "outputs": [],
   "source": [
    "def get_tables(param, baseUrl, dt_inicio_unix, dtIngestao, location_landing):\n",
    "    end_point_page = None\n",
    "    try:\n",
    "        format_timestamp = '%Y-%m-%d %H:%M:%S.%f'\n",
    "        dtIngestao = str(dtIngestao)\n",
    "        if param.vl_Ultimo_Incremento is not None:\n",
    "            vlUltimoIncremento = datetime.strptime(param.vl_Ultimo_Incremento, format_timestamp)\n",
    "            vlUltimoIncremento = int(mktime(vlUltimoIncremento.timetuple()))\n",
    "        else:\n",
    "            vlUltimoIncremento = dt_inicio_unix\n",
    "\n",
    "        vlUltimoIncremento = str(vlUltimoIncremento)\n",
    "        # vlUltimoIncremento = str(1685577600)\n",
    "\n",
    "        # Declarando as variáveis\n",
    "        pagina = 1\n",
    "        max_errors = 100\n",
    "        error_sleep = 60\n",
    "        inicio = time.time()\n",
    "\n",
    "        # Variáveis utilizadas apenas por algumas cargas, inicia com valores iguais para não impactar as tabelas que não utilizam\n",
    "        count = -1\n",
    "        count_api = -1\n",
    "\n",
    "        # Landing\n",
    "        if param.vl_Schedule_Carga != 0:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "        else:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "        \n",
    "        ## ALGUNS ENDPOINTS NÃO SUPORTAM A PAGINAÇÃO POR CURSOR.    \n",
    "        # PARA ATIVAR NOS QUE POSSUEM SUPORTE, É SÓ ADICIONAR \"?page[size]=100\" NO ENDPOINT\n",
    "        # https://developer.zendesk.com/api-reference/introduction/pagination/\n",
    "        print(f\"Tabela {param.ds_Nome_Arquivo_Landing}! Começando carga...\")\n",
    "\n",
    "        # Cria um objeto de lista para salvar os dados dessa iteração.\n",
    "        result_data = []\n",
    "\n",
    "        # Pega o endpoint que utilizará no request\n",
    "        # Verificação para casos especiais que salvam a última URL que processou\n",
    "        if param.ds_Custom_Field is None:\n",
    "            end_point_page = f\"{baseUrl}/{param.ds_Url}\".replace(\"@DT_INI_CARGA\", vlUltimoIncremento)\n",
    "        else:\n",
    "            end_point_page = param.ds_Custom_Field\n",
    "\n",
    "        # Envia a primeira requisição\n",
    "        req = requests.get(end_point_page, auth=HTTPBasicAuth(f\"{username}\", f\"{password}\"))\n",
    "        last_endpoint = end_point_page\n",
    "        print(last_endpoint)\n",
    "\n",
    "        # Verifica se a primeira requisição teve erro\n",
    "        if req.status_code != 200:\n",
    "            print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na primeira requisição!\")\n",
    "        # Caso a response da requisição seja 200 (sucesso) \n",
    "        else:\n",
    "            # Armazena o conteúdo da API na variável data em formato JSON\n",
    "            data = req.json()\n",
    "            data_keys = list(data.keys())\n",
    "\n",
    "            end_point_page = None\n",
    "            if 'meta' in data_keys:\n",
    "                if 'has_more' in list(data['meta'].keys()):\n",
    "                    if data['meta']['has_more'] == True:\n",
    "                        try:\n",
    "                            # Tenta capturar o campo \"links: next\" na API para consultar a próxima página, caso não conseguir, retorna erro\n",
    "                            end_point_page = data['links']['next']\n",
    "                        except:\n",
    "                            raise Exception(f'Não foi possível encontrar a próxima página para a tabela --> {param.ds_Nome_Arquivo_Landing}\\nPágina --> {pagina}')\n",
    "            \n",
    "            if 'next_page' in data_keys:\n",
    "                if 'count' in data_keys:\n",
    "                    if data['count'] < 50:\n",
    "                        end_point_page = None\n",
    "                    else:\n",
    "                        end_point_page = data['next_page']\n",
    "                \n",
    "            if  'count' in data_keys:\n",
    "                # Captura o campo \"count\" na API para verificar o tamanho padrão dos registros\n",
    "                count_api = data['count']\n",
    "                # Cria uma variável com o valor igual o da API que vai ser modificada a cada requisição obtendo o novo valor do campo \"count\"\n",
    "                count = count_api\n",
    "\n",
    "            # Caso tenha o campo end_of_steam, utiliza ele para definir o término do incremental\n",
    "            if 'end_of_stream' in data_keys:\n",
    "                end_stream = data['end_of_stream']\n",
    "                print(\"End_stream\", end_stream)\n",
    "            else:\n",
    "                end_stream = False\n",
    "\n",
    "            # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "            if param.nm_Item_Origem in ('ticket_metrics'):\n",
    "                result_data.extend(data['metric_sets'])\n",
    "            else:\n",
    "                result_data.extend(data[param.nm_Item_Origem])\n",
    "\n",
    "            if len(result_data) == 0: # caso a quantidade de registros seja 0 (vazio), encerra a execução para evitar erros no LOG\n",
    "                return\n",
    "\n",
    "            # Atualiza a contagem de páginas lidas\n",
    "            pagina += 1\n",
    "\n",
    "            # Atualiza o ds_Custom_Field para tabelas que utilizam a coluna\n",
    "            if param.ds_Custom_Field is not None:\n",
    "                fn_AtualizaCustomField_API(param.id_Parametro_Carga_API, last_endpoint)\n",
    "\n",
    "        # Faz requisições até que termine a paginação completa\n",
    "        inicio_laco = time.time()\n",
    "        while end_point_page is not None:\n",
    "            if end_stream == True:\n",
    "                # Caso o campo \"end_of_stream\" seja igual a True não percorre mais páginas\n",
    "                break\n",
    "\n",
    "            if count != count_api:\n",
    "                # Faz requisições até que o campo \"count\" da API venha com valor diferente do padrão da primeira página, significa que todas as páginas foram percorridas\n",
    "                break\n",
    "\n",
    "            # Envia a requisição para a página seguinte\n",
    "            req = requests.get(end_point_page, auth=HTTPBasicAuth(f\"{username}\", f\"{password}\"))\n",
    "            last_endpoint = end_point_page\n",
    "\n",
    "            # Em caso de erro entra no laço\n",
    "            if req.status_code != 200:\n",
    "                # Caso erro: printa o erro e a página, espera a quantidade de tempo declarada no \"error_sleep\" e confere se tiveram mais de 10 erros\n",
    "                print(\"\\n\"+f\"Tabela {param.nm_Item_Origem} com erro na requisição da página {pagina}\")\n",
    "                print(str(req.content))\n",
    "                max_errors -= 1\n",
    "                if(max_errors < 0):\n",
    "                    raise Exception(str(req.content))\n",
    "\n",
    "                sleep(error_sleep)\n",
    "            # Caso a response da requisição seja 200 (sucesso): Salva o conteúdo na variável \"data\", captura o campo \"next_page\" e armazena todo o conteúdo na lista dinâmica\n",
    "            else:\n",
    "                data = req.json()\n",
    "                data_keys = list(data.keys())\n",
    "\n",
    "                # Verifica todas as situações de paginação\n",
    "                end_point_page = None\n",
    "                if 'meta' in data_keys:\n",
    "                    if 'has_more' in list(data['meta'].keys()):\n",
    "                        if data['meta']['has_more'] == True:\n",
    "                            try:\n",
    "                                # Tenta capturar o campo \"links: next\" na API para consultar a próxima página, caso não conseguir, retorna erro\n",
    "                                end_point_page = data['links']['next']\n",
    "                            except:\n",
    "                                raise Exception(f'Não foi possível encontrar a próxima página para a tabela --> {param.ds_Nome_Arquivo_Landing}\\nPágina --> {pagina}')\n",
    "                \n",
    "                if 'next_page' in data_keys:\n",
    "                    # Tenta capturar o campo \"next_page\" na API para verificar se tem mais de uma página, caso não, considera o \"next_page\" igual a Null\n",
    "                    end_point_page = data['next_page']\n",
    "\n",
    "                if 'count' in data_keys:\n",
    "                    # Captura o campo \"count\" na API para verificar o tamanho padrão dos registros\n",
    "                    count_api = data['count']\n",
    "                    print(\"Count...\", count_api, count)\n",
    "\n",
    "                # Caso tenha o campo end_of_steam, utiliza ele para definir o término do incremental\n",
    "                if 'end_of_stream' in data_keys:\n",
    "                    end_stream = data['end_of_stream']\n",
    "                    print(\"End_stream\", end_stream)\n",
    "                else:\n",
    "                    end_stream = False\n",
    "\n",
    "                # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "                if param.nm_Item_Origem in ('ticket_metrics'):\n",
    "                    result_data.extend(data['metric_sets'])\n",
    "                else:\n",
    "                    result_data.extend(data[param.nm_Item_Origem])\n",
    "                    print(f\"Tabela {param.nm_Item_Origem} ---> Página {pagina} > {len(data[param.nm_Item_Origem])} registros\")\n",
    "\n",
    "                # Atualiza a contagem de páginas lidas\n",
    "                pagina+=1\n",
    "                print(\"Páginas processadas:\", pagina)\n",
    "                # Atualiza o ds_Custom_Field para tabelas que utilizam a coluna\n",
    "                if param.ds_Custom_Field is not None:\n",
    "                    fn_AtualizaCustomField_API(param.id_Parametro_Carga_API, last_endpoint)\n",
    "\n",
    "            fim_laco = time.time()\n",
    "            if (fim_laco - inicio_laco) > 7200:\n",
    "                raise Exception(f'A quantidade de tempo determinada foi excedida!\\nErro na tabela full --> {param.ds_Nome_Arquivo_Landing}\\nPágina --> {pagina}')\n",
    "\n",
    "        fim = time.time()\n",
    "        \n",
    "        # Subtrai a variável \"início\" pela \"fim\" para obter o tempo total de execução da tabela e armazena na váriavel \"tempo_exec\"\n",
    "        tempo_exec = fim - inicio\n",
    "\n",
    "        print(\"\\n\"+f\"A tabela {param.nm_Item_Origem} teve {pagina} páginas carregadas com êxito!\")\n",
    "        print(f\"Tamanho total de registros {len(result_data)}\")\n",
    "        print(f\"O tempo de execução da tabela {param.nm_Item_Origem} foi {tempo_exec}\")\n",
    "\n",
    "        # PROCESSO DE CARGA DOS DADOS NO BLOB\n",
    "        fn_SaveJson(result_data, dir_landing, str(param.ds_Nome_Arquivo_Landing).lower())\n",
    "\n",
    "        fn_AtualizaUltimoIncremento_API(param.id_Parametro_Carga_API, dtIngestao)\n",
    "\n",
    "        ## Constrói novos eventos a partir do evento principal da requisição da API\n",
    "\n",
    "        # Tabela Child_Events\n",
    "        if param.nm_Item_Origem == 'ticket_events':\n",
    "            result_child_events  = []\n",
    "            for data in result_data:\n",
    "                event_child = data['child_events']\n",
    "                for i in range(len(event_child)):\n",
    "                    compo_event_child = event_child[i]\n",
    "                    compo_event_child['ticket_id'] = data['ticket_id']\n",
    "                    compo_event_child['created_at'] = data['created_at']\n",
    "                    result_child_events.append(compo_event_child)\n",
    "            \n",
    "            print(\"Criando a tabela child_events...\")\n",
    "            fn_SaveJson(result_child_events, alter_landing.replace(\"@alter_table\", \"child_events\"), \"child_events\")\n",
    "            fn_AtualizaUltimoIncremento_API(314, dtIngestao)\n",
    "\n",
    "        # Tabela custom_field_options\n",
    "        if param.nm_Item_Origem == 'ticket_fields':\n",
    "            result_field_options = []\n",
    "            for data in result_data:\n",
    "                if 'custom_field_options' in list(data.keys()):\n",
    "                    field_options = data['custom_field_options']\n",
    "                    for i in range(len(field_options)):\n",
    "                        compo_field_options = field_options[i]\n",
    "                        compo_field_options['fields_id'] = data['id']\n",
    "                        result_field_options.append(compo_field_options)\n",
    "\n",
    "            print(\"Criando a tabela custom_field_options...\")\n",
    "            fn_SaveJson(result_field_options, alter_landing.replace(\"@alter_table\", \"custom_field_options\"), \"custom_field_options\")\n",
    "            fn_AtualizaUltimoIncremento_API(316, dtIngestao)\n",
    "    \n",
    "        ## LOG SUCESSO\n",
    "        dtFim = datetime.today() - timedelta(hours=3)\n",
    "        dtFim_format = dtFim.strftime(format_log)\n",
    "        duracao = int((dtFim-dtInicio).total_seconds()) #captura a duração subtraindo o dtFim pelo dtInicio\n",
    "        dsParametro = str(param.asDict()) #captura todos os parâmetros\n",
    "        notebook = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        \n",
    "        try:\n",
    "            pipeline = param.nm_Pipeline\n",
    "        except:\n",
    "            pipeline = os.path.basename(notebook)\n",
    "\n",
    "        if end_point_page is not None:\n",
    "            query = end_point_page\n",
    "        else:\n",
    "            query = ' '\n",
    "            \n",
    "        parametros = {\"tipo_log\": tipo_log,\"id_parametro\": param.id_Parametro_Carga_API, \"camada\": camada, \"dtInicio\": dtInicio_format, \"dtFim\": dtFim_format, \"pipeline\": pipeline, \"atividade\": atividade, \"notebook\": notebook, \"origem\": origem, \"destino\": destino, \"sistema\": param.nm_Sistema, \"emissor\": emissor, \"duracao\": duracao, \"query\": query, \"dsParametro\": dsParametro, \"execUrl\": execUrl}\n",
    "\n",
    "        fn_LogSucceeded(parametros, dt_ingestao.strftime(format_log))\n",
    "\n",
    "        return f\"Carga da tabela {param.ds_Nome_Arquivo_Landing} finalizada com sucesso.\"\n",
    "\n",
    "    except Exception as error:\n",
    "        ## LOG ERRO\n",
    "        dtFim = datetime.today() - timedelta(hours=3)\n",
    "        dtFim_format = dtFim.strftime(format_log)\n",
    "        duracao = int((dtFim-dtInicio).total_seconds()) #captura a duração subtraindo o dtFim pelo dtInicio\n",
    "        dsParametro = str(param.asDict()) #captura todos os parâmetros\n",
    "        notebook = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        try:\n",
    "            pipeline = param.nm_Pipeline\n",
    "        except:\n",
    "            pipeline = os.path.basename(notebook)\n",
    "\n",
    "        if end_point_page is not None:\n",
    "            query = end_point_page\n",
    "        else:\n",
    "            query = ' '\n",
    "\n",
    "        if hasattr(error, 'code'): # captura o código do erro, caso possua o atributo 'code'\n",
    "            error_code = error.code\n",
    "        else:\n",
    "            error_code = 'NULL'\n",
    "            \n",
    "        parametros = {\"tipo_log\": tipo_log,\"id_parametro\": param.id_Parametro_Carga_API, \"camada\": camada, \"dtInicio\": dtInicio_format, \"dtFim\": dtFim_format, \"pipeline\": pipeline, \"atividade\": atividade, \"notebook\": notebook, \"origem\": origem, \"destino\": destino, \"sistema\": param.nm_Sistema, \"emissor\": emissor, \"duracao\": duracao, \"query\": query, \"dsParametro\": dsParametro, \"cd_erro\": error_code, \"erro\": str(error), \"execUrl\": execUrl}\n",
    "\n",
    "        fn_LogFailed(parametros, dt_ingestao.strftime(format_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4899a1-bb57-41a4-a157-ce531df60303",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cria uma lista para armazenar todas as tarefas\n",
    "tasks = []\n",
    "\n",
    "# Cria uma instância do ThreadPoolExecutor com threads definidas (max_workers)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "    # Percorre todas as pastas do diretório de origem\n",
    "    for row in data_param:\n",
    "        # Executa a função fn_StreamFromFolder_csv em uma thread do ThreadPoolExecutor\n",
    "        task = executor.submit(get_tables, *(row, baseUrl, dt_inicio_unix, dt_fim, location_landing))\n",
    "                               \n",
    "        # Adiciona a tarefa à lista de tarefas\n",
    "        tasks.append(task)\n",
    "\n",
    "# Aguarda a conclusão de todas as tarefas\n",
    "_ = concurrent.futures.wait(tasks, return_when='ALL_COMPLETED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea021407-3a7d-4c20-8b47-8813d762106b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    try:\n",
    "        print(task.result(),'\\n')\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Process_Ingestao_VR_Gente_Zendesk",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
