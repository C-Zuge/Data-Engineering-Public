{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c877e4e2-7658-4503-86ae-593ee236b515",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Utils/Functions/core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a6fe3e-6e4c-4060-9951-1d6775c77f04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from pyspark.sql.functions import col\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import concurrent.futures\n",
    "from time import sleep, mktime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e084cd59-a7b0-4bc1-a471-209729c3b791",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Cria as variáveis de LOG\n",
    "format_log = '%Y-%m-%d %H:%M:%S'\n",
    "dtInicio = datetime.today() - timedelta(hours=3)\n",
    "dtInicio_format = dtInicio.strftime(format_log)\n",
    "tipo_log = 'API'\n",
    "camada = 'landing'\n",
    "emissor = '<org>'\n",
    "atividade = '<activity_desc>'\n",
    "origem = 'RestService'\n",
    "destino = 'AzureBlobFS'\n",
    "execUrl = ' '\n",
    "\n",
    "try:\n",
    "    infos = json.loads(dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson()) # captura as informações do job que executa o notebook\n",
    "    orgId = infos['tags']['orgId']\n",
    "    runId = infos['tags']['multitaskParentRunId']\n",
    "    jobId = infos['tags']['jobId']\n",
    "    if orgId == '2960871991268730': # Monta a URL caso seja o ID do ambiente de DEV\n",
    "        execUrl = f'https://adb-{orgId}.10.azuredatabricks.net/?o={orgId}#job/{jobId}/run/{runId}' # cria a url de execução do \n",
    "    else: # Monta a URL caso seja o ID do ambiente de PROD\n",
    "        execUrl = f'https://adb-{orgId}.15.azuredatabricks.net/?o={orgId}#job/{jobId}/run/{runId}' # cria a url de execução do \n",
    "except:\n",
    "    print('Campo de URL não pode ser identificado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9450d679-1b9f-4018-bcb2-b0efe4caeae2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog prod;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eb9b5e5-3853-4124-a0be-547fe13fb3a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"dt_ingestao\", \"\")\n",
    "\n",
    "dt_ingestao = getArgument(\"dt_ingestao\").upper().strip()\n",
    "\n",
    "location_landing = spark.sql(\"show external locations\").select(\"url\").where(\"name = 'landing-area'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b8d1860-f143-4ea7-911d-ff554173004b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Formata o dt_ingestao\n",
    "format_timestamp = '%Y-%m-%d %H:%M:%S.%f'\n",
    "dt_ingestao = datetime.now() if dt_ingestao == \"\" else datetime.strptime(dt_ingestao, format_timestamp)\n",
    "\n",
    "# Pega o horário atual e tira 3 horas para converter para BRT (UTC -3)\n",
    "# Tira 5 minutos por conta de um limite da API\n",
    "dt_fim = dt_ingestao - timedelta(hours=3) - timedelta(minutes=5)\n",
    "dt_inicio = dt_fim - timedelta(days=1)\n",
    "\n",
    "# Força as horas, minutos e segundos em 0\n",
    "dt_inicio = datetime(dt_inicio.year, dt_inicio.month, dt_inicio.day, 0, 0, 0)\n",
    "\n",
    "# Separa as datas no formato UNIX\n",
    "dt_fim_unix = int(mktime(dt_fim.timetuple()))\n",
    "dt_inicio_unix = int(mktime(dt_inicio.timetuple()))\n",
    "\n",
    "# dt_inicio_unix = 1546300800\n",
    "\n",
    "# Transforma as datas em STRING utilizando um formato de timestamp\n",
    "dt_fim = dt_fim.strftime(format_timestamp)\n",
    "dt_inicio = dt_inicio.strftime(format_timestamp)\n",
    "\n",
    "print(f\"dt_inicio: {dt_inicio} | unix: {dt_inicio_unix}\")\n",
    "print(f\"dt_fim   : {dt_fim} | unix: {dt_fim_unix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5778bee4-e0b0-4f8b-bc0c-be06c75fa0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##CONFIGURAÇÕES DE PROCESSO DE CARGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c54390dc-59d1-4203-b383-cd70cf9508d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user = dbutils.secrets.get(\"scope-vault-data\", \"jira-user\")\n",
    "password  = dbutils.secrets.get(\"scope-vault-data\", \"jira-api-token\")\n",
    "domain = \"<domain>\"\n",
    "subdomain = \"atlassian\"\n",
    "baseUrl = f\"https://{domain}.{subdomain}.net\"\n",
    "auth = HTTPBasicAuth(f\"{user}\", f\"{password}\")\n",
    "\n",
    "threads = 10    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7837fb9-f078-40c1-ba16-891b84b9f2dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_param = fn_ConsultaJdbc(\"\"\"\n",
    "    SELECT pca.*\n",
    "    FROM ctl.ADF_Parametro_Carga_API pca\n",
    "    WHERE pca.fl_ativo = 1\n",
    "    AND nm_Sistema = 'jira'\n",
    "    AND (ds_Custom_Field != 'id' or ds_Custom_Field is null)\n",
    "    AND pca.id_Parametro_Carga_API not in (579)\n",
    "\"\"\")\n",
    "\n",
    "display(df_param)\n",
    "data_param = df_param.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709dc125-8bc1-4ff3-95c5-e4e79e4798f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_param_id = fn_ConsultaJdbc(\"\"\"\n",
    "    SELECT pca.*\n",
    "    FROM ctl.ADF_Parametro_Carga_API pca\n",
    "    WHERE pca.fl_ativo = 1\n",
    "    AND nm_Sistema = 'jira' and ds_Custom_Field = 'id'\n",
    "\"\"\")\n",
    "\n",
    "display(df_param_id)\n",
    "data_param_id = df_param_id.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f1a5f05-edfe-4cd8-9c32-945b3c693256",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SCRIPT DA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6f9cbd6-5a05-4c59-8603-822541c7231c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_tables_from_ids(param, baseUrl, dt_inicio_unix, dtIngestao, location_landing, ids, id_tabela):\n",
    "    end_point_page = None\n",
    "    try:\n",
    "        # Declarando as variáveis\n",
    "        inicio = time.time()\n",
    "\n",
    "        if param.vl_Schedule_Carga != 0:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "        else:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "        \n",
    "        data_format = []\n",
    "        for id in ids:\n",
    "            start_at = None\n",
    "            total = None\n",
    "            is_last = None\n",
    "            max_results = None\n",
    "            pagina = 1\n",
    "            print(f\"Tabela {param.ds_Nome_Arquivo_Landing} ---> ID: {id}! Começando carga...\")\n",
    "\n",
    "            # Pega o endpoint que utilizará no request\n",
    "            # Verificação para casos especiais que salvam a última URL que processou\n",
    "            end_point_page = baseUrl+param.ds_Url.replace('@id', str(id))\n",
    "            req = requests.get(end_point_page, auth=auth)\n",
    "            data = req.json()\n",
    "            data_list = []\n",
    "\n",
    "            # Verifica se a primeira requisição teve erro\n",
    "            if req.status_code != 200:\n",
    "                print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na primeira requisição do ID {id}!\")\n",
    "            # Caso a response da requisição seja 200 (sucesso) \n",
    "            else:\n",
    "                # Armazena o conteúdo da API na variável data em formato JSON\n",
    "                data = req.json()\n",
    "\n",
    "                if type(data) != dict:\n",
    "                    data = {}\n",
    "                    data[param.nm_Item_Origem] = req.json()\n",
    "\n",
    "                end_point_page = None\n",
    "                data_keys = list(data.keys())\n",
    "\n",
    "                # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "                if param.nm_Item_Origem == 'issue_label':\n",
    "                    data_list.append(data)\n",
    "                elif param.nm_Definition is not None:\n",
    "                    campo_data = param.nm_Definition.split(',')\n",
    "                    if len(campo_data) == 2:\n",
    "                        data_list.extend(data[campo_data[0]][campo_data[1]])\n",
    "                    else:\n",
    "                        data_list.extend(data[campo_data[0]])\n",
    "                else:\n",
    "                    data_list.extend(data[param.nm_Item_Origem])\n",
    "\n",
    "                if 'startAt' and 'maxResults' in data_keys:\n",
    "                    start_at = data['startAt']\n",
    "                    max_results = data['maxResults']\n",
    "                    start_at += max_results\n",
    "                    end_point_page = baseUrl+param.ds_Url.replace('@id', str(id))+f'&startAt={start_at}'\n",
    "                if 'total' in data_keys:\n",
    "                    total = data['total']\n",
    "                elif 'isLast' in data_keys:\n",
    "                    is_last = data['isLast']\n",
    "\n",
    "                print(f'Tabela {param.nm_Item_Origem} ---> ID: {id} teve {pagina} páginas adicionadas com êxito!')\n",
    "                    \n",
    "            while (total != None and (total - start_at) >= 0 or is_last != None and is_last == False):\n",
    "                req = requests.get(end_point_page, auth=auth)\n",
    "                data = req.json()\n",
    "                pagina += 1\n",
    "\n",
    "                # Verifica se a primeira requisição teve erro\n",
    "                if req.status_code != 200:\n",
    "                    print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na página {pagina}!\")\n",
    "                    pagina -= 1\n",
    "                # Caso a response da requisição seja 200 (sucesso) \n",
    "                else:\n",
    "                    # Armazena o conteúdo da API na variável data em formato JSON\n",
    "                    data = req.json()\n",
    "\n",
    "                    if type(data) != dict:\n",
    "                        data = {}\n",
    "                        data[param.nm_Item_Origem] = req.json()\n",
    "\n",
    "                    end_point_page = None\n",
    "\n",
    "                    data_keys = list(data.keys())\n",
    "\n",
    "                    # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "                    if param.nm_Item_Origem == 'issue_label':\n",
    "                        data_list.append(data)\n",
    "                    elif param.nm_Definition is not None:\n",
    "                        campo_data = param.nm_Definition.split(',')\n",
    "                        if len(campo_data) == 2:\n",
    "                            data_list.extend(data[campo_data[0]][campo_data[1]])\n",
    "                        else:\n",
    "                            data_list.extend(data[campo_data[0]])\n",
    "                    else:\n",
    "                        data_list.extend(data[param.nm_Item_Origem])\n",
    "\n",
    "                    if 'startAt' and 'maxResults' in data_keys:\n",
    "                        start_at = data['startAt']\n",
    "                        max_results = data['maxResults']\n",
    "                        start_at += max_results\n",
    "                        end_point_page = baseUrl+param.ds_Url.replace('@id', str(id))+f'&startAt={start_at}'\n",
    "                    if 'total' in data_keys:\n",
    "                        is_last = None\n",
    "                        total = data['total']\n",
    "                    elif 'isLast' in data_keys:\n",
    "                        total = None\n",
    "                        is_last = data['isLast']\n",
    "                    \n",
    "                    print(f'Tabela {param.nm_Item_Origem} ---> ID: {id} teve {pagina} páginas adicionadas com êxito!')\n",
    "\n",
    "            for data in data_list:\n",
    "                data.update({id_tabela: str(id)})\n",
    "                if param.nm_Item_Origem == 'issue_label':\n",
    "                    label = data['fields']['labels']\n",
    "                    data.pop('fields')\n",
    "                    data.update({'label': label})\n",
    "                data_format.append(data)\n",
    "\n",
    "        if len(data_format) == 0: # caso a quantidade de registros seja 0 (vazio), encerra a execução para evitar erros no LOG\n",
    "            return\n",
    "\n",
    "        fim = time.time()\n",
    "        \n",
    "        # Subtrai a variável \"início\" pela \"fim\" para obter o tempo total de execução da tabela e armazena na váriavel \"tempo_exec\"\n",
    "        tempo_exec = fim - inicio\n",
    "\n",
    "        print(\"\\n\"+f\"A tabela {param.nm_Item_Origem} teve {pagina} páginas carregadas com êxito!\")\n",
    "        print(f\"Tamanho total de registros {len(data_format)}\")\n",
    "        print(f\"O tempo de execução da tabela {param.nm_Item_Origem} foi {tempo_exec}\")\n",
    "\n",
    "        # PROCESSO DE CARGA DOS DADOS NO BLOB\n",
    "        fn_SaveJson(data_format, dir_landing, str(param.ds_Nome_Arquivo_Landing).lower())\n",
    "\n",
    "        fn_AtualizaUltimoIncremento_API(param.id_Parametro_Carga_API, dtIngestao)\n",
    "\n",
    "        ## LOG SUCESSO\n",
    "        dtFim = datetime.today() - timedelta(hours=3)\n",
    "        dtFim_format = dtFim.strftime(format_log)\n",
    "        duracao = int((dtFim-dtInicio).total_seconds()) #captura a duração subtraindo o dtFim pelo dtInicio\n",
    "        dsParametro = str(param.asDict()) #captura todos os parâmetros\n",
    "        notebook = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        \n",
    "        try:\n",
    "            pipeline = param.nm_Pipeline\n",
    "        except:\n",
    "            pipeline = os.path.basename(notebook)\n",
    "\n",
    "        if end_point_page is not None:\n",
    "            query = end_point_page\n",
    "        else:\n",
    "            query = ' '\n",
    "            \n",
    "        parametros = {\"tipo_log\": tipo_log,\"id_parametro\": param.id_Parametro_Carga_API, \"camada\": camada, \"dtInicio\": dtInicio_format, \"dtFim\": dtFim_format, \"pipeline\": pipeline, \"atividade\": atividade, \"notebook\": notebook, \"origem\": origem, \"destino\": destino, \"sistema\": param.nm_Sistema, \"emissor\": emissor, \"duracao\": duracao, \"query\": query, \"dsParametro\": dsParametro, \"execUrl\": execUrl}\n",
    "\n",
    "        fn_LogSucceeded(parametros, dt_ingestao.strftime(format_log))\n",
    "\n",
    "    except Exception as error:\n",
    "        ## LOG ERRO\n",
    "        dtFim = datetime.today() - timedelta(hours=3)\n",
    "        dtFim_format = dtFim.strftime(format_log)\n",
    "        duracao = int((dtFim-dtInicio).total_seconds()) #captura a duração subtraindo o dtFim pelo dtInicio\n",
    "        dsParametro = str(param.asDict()) #captura todos os parâmetros\n",
    "        notebook = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        try:\n",
    "            pipeline = param.nm_Pipeline\n",
    "        except:\n",
    "            pipeline = os.path.basename(notebook)\n",
    "\n",
    "        if end_point_page is not None:\n",
    "            query = end_point_page\n",
    "        else:\n",
    "            query = ' '\n",
    "\n",
    "        if hasattr(error, 'code'): # captura o código do erro, caso possua o atributo 'code'\n",
    "            error_code = error.code\n",
    "        else:\n",
    "            error_code = 'NULL'\n",
    "            \n",
    "        parametros = {\"tipo_log\": tipo_log,\"id_parametro\": param.id_Parametro_Carga_API, \"camada\": camada, \"dtInicio\": dtInicio_format, \"dtFim\": dtFim_format, \"pipeline\": pipeline, \"atividade\": atividade, \"notebook\": notebook, \"origem\": origem, \"destino\": destino, \"sistema\": param.nm_Sistema, \"emissor\": emissor, \"duracao\": duracao, \"query\": query, \"dsParametro\": dsParametro, \"cd_erro\": error_code, \"erro\": str(error), \"execUrl\": execUrl}\n",
    "\n",
    "        fn_LogFailed(parametros, dt_ingestao.strftime(format_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6a918f8-61ae-4c14-aacd-824d0ec2cd93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_tables(param, baseUrl, dt_inicio_unix, dtIngestao, location_landing):\n",
    "    end_point_page = None\n",
    "    try:\n",
    "        format_timestamp = '%Y-%m-%d %H:%M:%S.%f'\n",
    "        dtIngestao = str(dtIngestao)\n",
    "        if param.vl_Ultimo_Incremento is not None:\n",
    "            vlUltimoIncremento = datetime.strptime(param.vl_Ultimo_Incremento, format_timestamp)\n",
    "            vlUltimoIncremento = int(mktime(vlUltimoIncremento.timetuple()))\n",
    "        else:\n",
    "            vlUltimoIncremento = dt_inicio_unix\n",
    "\n",
    "        vlUltimoIncremento = str(vlUltimoIncremento)\n",
    "\n",
    "        # Declarando as variáveis\n",
    "        start_at = None\n",
    "        total = None\n",
    "        is_last = None\n",
    "        max_results = None\n",
    "        pagina = 1\n",
    "        inicio = time.time()\n",
    "\n",
    "        if param.vl_Schedule_Carga != 0:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "        else:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "\n",
    "        print(f\"Tabela {param.ds_Nome_Arquivo_Landing}! Começando carga...\")\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        # Pega o endpoint que utilizará no request\n",
    "        # Verificação para casos especiais que salvam a última URL que processou\n",
    "        if param.ds_Custom_Field is None:\n",
    "            end_point_page = f\"{baseUrl}{param.ds_Url}\"\n",
    "        else:\n",
    "            end_point_page = baseUrl+param.ds_Custom_Field\n",
    "            # end_point_page = f\"{baseUrl}{param.ds_Url}\"\n",
    "        \n",
    "        req = requests.get(end_point_page, auth=auth)\n",
    "\n",
    "        # Verifica se a primeira requisição teve erro\n",
    "        if req.status_code != 200:\n",
    "            print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na primeira requisição!\")\n",
    "        # Caso a response da requisição seja 200 (sucesso) \n",
    "        else:\n",
    "            # Armazena o conteúdo da API na variável data em formato JSON\n",
    "            data = req.json()\n",
    "\n",
    "            if type(data) != dict:\n",
    "                data = {}\n",
    "                data[param.nm_Item_Origem] = req.json()\n",
    "                \n",
    "            data_keys = list(data.keys())\n",
    "\n",
    "            end_point_page = None\n",
    "\n",
    "            # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "            if param.nm_Definition is not None:\n",
    "                data_list.extend(data[param.nm_Definition])\n",
    "            else:\n",
    "                data_list.extend(data[param.nm_Item_Origem])\n",
    "\n",
    "            if len(data_list) == 0: # caso a quantidade de registros seja 0 (vazio), encerra a execução para evitar erros no LOG\n",
    "                return\n",
    "\n",
    "            if 'startAt' and 'maxResults' in data_keys:\n",
    "                start_at = data['startAt']\n",
    "                max_results = data['maxResults']\n",
    "                start_at += max_results\n",
    "                if param.ds_Custom_Field is None:\n",
    "                    end_point_page = baseUrl+param.ds_Url+f'&startAt={start_at}'\n",
    "                else:\n",
    "                    end_point_page = baseUrl+param.ds_Custom_Field+f'&startAt={start_at}'\n",
    "                    # end_point_page = baseUrl+param.ds_Url+f'&startAt={start_at}'\n",
    "            if 'total' in data_keys:\n",
    "                total = data['total']\n",
    "            elif 'isLast' in data_keys:\n",
    "                is_last = data['isLast']\n",
    "\n",
    "        while (total != None and (total - start_at) >= 0 or is_last != None and is_last == False):\n",
    "            req = requests.get(end_point_page, auth=auth)\n",
    "            data = req.json()\n",
    "            pagina += 1\n",
    "\n",
    "            # Verifica se a primeira requisição teve erro\n",
    "            if req.status_code != 200:\n",
    "                print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na página {pagina}!\")\n",
    "                pagina -= 1\n",
    "            # Caso a response da requisição seja 200 (sucesso) \n",
    "            else:\n",
    "                # Armazena o conteúdo da API na variável data em formato JSON\n",
    "                data = req.json()\n",
    "\n",
    "                if type(data) != dict:\n",
    "                    data = {}\n",
    "                    data[param.nm_Item_Origem] = req.json()\n",
    "\n",
    "                end_point_page = None\n",
    "\n",
    "                data_keys = list(data.keys())\n",
    "\n",
    "                # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "                if param.nm_Definition is not None:\n",
    "                    data_list.extend(data[param.nm_Definition])\n",
    "                else:\n",
    "                    data_list.extend(data[param.nm_Item_Origem])\n",
    "\n",
    "                if 'startAt' and 'maxResults' in data_keys:\n",
    "                    start_at = data['startAt']\n",
    "                    max_results = data['maxResults']\n",
    "                    start_at += max_results\n",
    "                    if param.ds_Custom_Field is None:\n",
    "                        end_point_page = baseUrl+param.ds_Url+f'&startAt={start_at}'\n",
    "                    else:\n",
    "                        end_point_page = baseUrl+param.ds_Custom_Field+f'&startAt={start_at}'\n",
    "                        # end_point_page = baseUrl+param.ds_Url+f'&startAt={start_at}'\n",
    "                if 'total' in data_keys:\n",
    "                    total = data['total']\n",
    "                elif 'isLast' in data_keys:\n",
    "                    is_last = data['isLast']\n",
    "\n",
    "        data_format = data_list\n",
    "\n",
    "        fim = time.time()\n",
    "        \n",
    "        # Subtrai a variável \"início\" pela \"fim\" para obter o tempo total de execução da tabela e armazena na váriavel \"tempo_exec\"\n",
    "        tempo_exec = fim - inicio\n",
    "\n",
    "        print(\"\\n\"+f\"A tabela {param.nm_Item_Origem} teve {pagina} páginas carregadas com êxito!\")\n",
    "        print(f\"Tamanho total de registros {len(data_list)}\")\n",
    "        print(f\"O tempo de execução da tabela {param.nm_Item_Origem} foi {tempo_exec}\")\n",
    "\n",
    "        # PROCESSO DE CARGA DOS DADOS NO BLOB\n",
    "        fn_SaveJson(data_format, dir_landing, str(param.ds_Nome_Arquivo_Landing).lower())\n",
    "\n",
    "        fn_AtualizaUltimoIncremento_API(param.id_Parametro_Carga_API, dtIngestao)\n",
    "\n",
    "        ## LOG SUCESSO\n",
    "        dtFim = datetime.today() - timedelta(hours=3)\n",
    "        dtFim_format = dtFim.strftime(format_log)\n",
    "        duracao = int((dtFim-dtInicio).total_seconds()) #captura a duração subtraindo o dtFim pelo dtInicio\n",
    "        dsParametro = str(param.asDict()) #captura todos os parâmetros\n",
    "        notebook = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        \n",
    "        try:\n",
    "            pipeline = param.nm_Pipeline\n",
    "        except:\n",
    "            pipeline = os.path.basename(notebook)\n",
    "\n",
    "        if end_point_page is not None:\n",
    "            query = end_point_page\n",
    "        else:\n",
    "            query = ' '\n",
    "            \n",
    "        parametros = {\"tipo_log\": tipo_log,\"id_parametro\": param.id_Parametro_Carga_API, \"camada\": camada, \"dtInicio\": dtInicio_format, \"dtFim\": dtFim_format, \"pipeline\": pipeline, \"atividade\": atividade, \"notebook\": notebook, \"origem\": origem, \"destino\": destino, \"sistema\": param.nm_Sistema, \"emissor\": emissor, \"duracao\": duracao, \"query\": query, \"dsParametro\": dsParametro, \"execUrl\": execUrl}\n",
    "\n",
    "        fn_LogSucceeded(parametros, dt_ingestao.strftime(format_log))\n",
    "\n",
    "        # Tabela Board_Issue\n",
    "        if param.nm_Item_Origem in ['board', 'issue', 'project']:\n",
    "            list_ids = []\n",
    "            subtables_board = ['board_issue', 'board_project', 'sprint']\n",
    "            subtables_issue = ['issue_changelog','issue_comment','issue_link','issue_subtask','issue_watcher','issue_worklog','issue_label','issue_component','issue_fix_version','issue_version','issue_change_details']\n",
    "            subtables_project = ['version','component']\n",
    "\n",
    "            if param.nm_Item_Origem == 'board':\n",
    "                data_param_id = df_param_id.filter(col('nm_Item_Origem').isin(subtables_board)).collect()\n",
    "                id_tabela = 'board_id'\n",
    "            elif param.nm_Item_Origem == 'issue':\n",
    "                data_param_id = df_param_id.filter(col('nm_Item_Origem').isin(subtables_issue)).collect()\n",
    "                id_tabela = 'issue_id'\n",
    "            elif param.nm_Item_Origem == 'project':\n",
    "                data_param_id = df_param_id.filter(col('nm_Item_Origem').isin(subtables_project)).collect()\n",
    "                id_tabela = 'project_id'\n",
    "\n",
    "            for data in data_list:\n",
    "                list_ids.append(data['id'])\n",
    "\n",
    "            for row in data_param_id:\n",
    "                # Executa a função fn_StreamFromFolder_csv em uma thread do ThreadPoolExecutor\n",
    "                subtask = executor.submit(get_tables_from_ids, *(row, baseUrl, dt_inicio_unix, dt_fim, location_landing, list_ids,id_tabela))\n",
    "                # Adiciona a tarefa à lista de tarefas\n",
    "                subtasks.append(subtask)\n",
    "\n",
    "    except Exception as error:\n",
    "        ## LOG ERRO\n",
    "        dtFim = datetime.today() - timedelta(hours=3)\n",
    "        dtFim_format = dtFim.strftime(format_log)\n",
    "        duracao = int((dtFim-dtInicio).total_seconds()) #captura a duração subtraindo o dtFim pelo dtInicio\n",
    "        dsParametro = str(param.asDict()) #captura todos os parâmetros\n",
    "        notebook = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        try:\n",
    "            pipeline = param.nm_Pipeline\n",
    "        except:\n",
    "            pipeline = os.path.basename(notebook)\n",
    "\n",
    "        if end_point_page is not None:\n",
    "            query = end_point_page\n",
    "        else:\n",
    "            query = ' '\n",
    "\n",
    "        if hasattr(error, 'code'): # captura o código do erro, caso possua o atributo 'code'\n",
    "            error_code = error.code\n",
    "        else:\n",
    "            error_code = 'NULL'\n",
    "            \n",
    "        parametros = {\"tipo_log\": tipo_log,\"id_parametro\": param.id_Parametro_Carga_API, \"camada\": camada, \"dtInicio\": dtInicio_format, \"dtFim\": dtFim_format, \"pipeline\": pipeline, \"atividade\": atividade, \"notebook\": notebook, \"origem\": origem, \"destino\": destino, \"sistema\": param.nm_Sistema, \"emissor\": emissor, \"duracao\": duracao, \"query\": query, \"dsParametro\": dsParametro, \"cd_erro\": error_code, \"erro\": str(error), \"execUrl\": execUrl}\n",
    "\n",
    "        fn_LogFailed(parametros, dt_ingestao.strftime(format_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b91cfc9-1d77-44e0-a351-294df15641aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cria uma lista para armazenar todas as tarefas\n",
    "tasks = []\n",
    "subtasks = []\n",
    "\n",
    "# Cria uma instância do ThreadPoolExecutor com threads definidas (max_workers)\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=threads)\n",
    "\n",
    "# Percorre todas as pastas do diretório de origem\n",
    "for row in data_param:\n",
    "    # Executa a função fn_StreamFromFolder_csv em uma thread do ThreadPoolExecutor\n",
    "    task = executor.submit(get_tables, *(row, baseUrl, dt_inicio_unix, dt_fim, location_landing))\n",
    "    # Adiciona a tarefa à lista de tarefas\n",
    "    tasks.append(task)\n",
    "\n",
    "# Aguarda a conclusão de todas as tarefas\n",
    "_ = concurrent.futures.wait(tasks, return_when='ALL_COMPLETED')\n",
    "\n",
    "executor.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5035cded-7f62-43fc-981a-b584ef8d5bef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tarefas = tasks + subtasks\n",
    "\n",
    "for task in tarefas:\n",
    "    try:\n",
    "        print(task.result(),'\\n')\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Process_Ingestao_VR_Gente_Jira",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
