{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e720ca92-63f7-4265-b975-4bfa08d21959",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run Utils/Functions/core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "402fad82-52d8-425d-b994-f80dfa0c3f5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from pyspark.sql.functions import col\n",
    "from uuid import uuid4\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import concurrent.futures\n",
    "from time import sleep, mktime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6860c332-3f95-4a19-894a-124fb4319c59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog prod;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a68c4dfd-42dc-432c-bef9-cee45c7d069f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"dt_ingestao\", \"\")\n",
    "\n",
    "dt_ingestao = getArgument(\"dt_ingestao\").upper().strip()\n",
    "\n",
    "location_landing = spark.sql(\"show external locations\").select(\"url\").where(\"name = 'landing-area'\").collect()[0][0]\n",
    "location_flat_file = spark.sql(\"show external locations\").select(\"url\").where(\"name = 'flatfile-area'\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0433f538-d4bb-438c-846b-3f09af32d9f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Formata o dt_ingestao\n",
    "format_timestamp = '%Y-%m-%d %H:%M:%S.%f'\n",
    "format_date = '%Y-%m-%d'\n",
    "dt_ingestao = datetime.now() if dt_ingestao == \"\" else datetime.strptime(dt_ingestao, format_timestamp)\n",
    "\n",
    "# Pega o horário atual e tira 3 horas para converter para BRT (UTC -3)\n",
    "# Tira 5 minutos por conta de um limite da API\n",
    "dt_fim = dt_ingestao - timedelta(hours=3) - timedelta(minutes=5)\n",
    "dt_inicio = dt_fim - timedelta(days=1)\n",
    "\n",
    "# Força as horas, minutos e segundos em 0\n",
    "dt_inicio = datetime(dt_inicio.year, dt_inicio.month, dt_inicio.day, 0, 0, 0)\n",
    "\n",
    "# Separa as datas no formato UNIX\n",
    "dt_fim_unix = int(mktime(dt_fim.timetuple()))\n",
    "dt_inicio_unix = int(mktime(dt_inicio.timetuple()))\n",
    "\n",
    "# Transforma as datas em STRING no formato date para usar na API\n",
    "since = dt_inicio.strftime(format_date)\n",
    "until = dt_fim.strftime(format_date)\n",
    "\n",
    "# Transforma as datas em STRING utilizando um formato de timestamp\n",
    "dt_fim = dt_fim.strftime(format_timestamp)\n",
    "dt_inicio = dt_inicio.strftime(format_timestamp)\n",
    "\n",
    "print(f\"dt_inicio: {dt_inicio} | unix: {dt_inicio_unix}\")\n",
    "print(f\"dt_fim   : {dt_fim} | unix: {dt_fim_unix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3d804a-6a1f-444b-83f3-9c409c65b5cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##CONFIGURAÇÕES DE PROCESSO DE CARGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6a5eaed-8ffe-4521-9508-ea9c0951de36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "access_token = '<access_token>'\n",
    "params = {'access_token': f'{access_token}'}\n",
    "domain = \"graph.facebook.com\"\n",
    "version = \"v17.0\"\n",
    "baseUrl = f\"https://{domain}/{version}\"\n",
    "\n",
    "threads = 10    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f72dc49-3152-45e4-83e0-1b2990175316",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_param = fn_ConsultaJdbc(\"\"\"\n",
    "    SELECT pca.*\n",
    "    FROM ctl.ADF_Parametro_Carga_API pca\n",
    "    WHERE pca.fl_ativo = 1\n",
    "    and nm_Sistema = 'facebook_ads'\n",
    "    and (ds_Custom_Field != 'id' or ds_Custom_Field is null)\n",
    "\"\"\")\n",
    "\n",
    "display(df_param)\n",
    "data_param = df_param.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ae6348e-51da-4226-a0c6-3db0749033f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_param_id = fn_ConsultaJdbc(\"\"\"\n",
    "    SELECT pca.*\n",
    "    FROM ctl.ADF_Parametro_Carga_API pca\n",
    "    WHERE pca.fl_ativo = 1\n",
    "    and nm_Sistema = 'facebook_ads' and ds_Custom_Field = 'id'\n",
    "\"\"\")\n",
    "\n",
    "display(df_param_id)\n",
    "data_param_id = df_param_id.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2baf0384-8d43-4755-abea-4992edb96d10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SCRIPT DA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad941fc-7ecb-4f78-8d91-5f6cb1bdfba7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_tables_from_ids(param, baseUrl, dt_inicio_unix, dtIngestao, location_landing, ids, act_ids):\n",
    "    try:\n",
    "        inicio = time.time()\n",
    "\n",
    "        if param.vl_Schedule_Carga != 0:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "        else:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "        \n",
    "        data_format = []\n",
    "        for id,act_id in zip(ids,act_ids):\n",
    "            # Declarando as variáveis\n",
    "            next = None\n",
    "            limite = 200\n",
    "            pagina = 1\n",
    "            error_sleep = 60\n",
    "\n",
    "            print(f\"Tabela {param.ds_Nome_Arquivo_Landing} ---> ID: {id} | {act_id} <--- ACCOUNT ID! Começando carga...\")\n",
    "\n",
    "            # Pega o endpoint que utilizará no request\n",
    "            # Verificação para casos especiais que salvam a última URL que processou\n",
    "            end_point_page = baseUrl+param.ds_Url.replace('@accountId', str(id)).replace('@dtInicio', since).replace('@dtFim', until)\n",
    "            req = requests.get(end_point_page, params=params)\n",
    "            data = req.json()\n",
    "            data_list = []\n",
    "            consumo = dict(req.headers)\n",
    "\n",
    "            try:\n",
    "                cpu = consumo['x-business-use-case-usage']\n",
    "                cpu = json.loads(cpu)\n",
    "                consumo_cpu = int(cpu[str(act_id)][0]['total_cputime'])\n",
    "                consumo_time = int(cpu[str(act_id)][0]['total_time'])\n",
    "                \n",
    "                if consumo_cpu > 80:\n",
    "                    print(f'Consumo excessivo de CPU da origem! Esperando um pouco antes de continuar o processamento.\\nNível da CPU ---> {consumo_cpu}')\n",
    "                    sleep(1200)\n",
    "                elif consumo_time > 80:\n",
    "                    print(f'Consumo excessivo de tempo da origem! Esperando um pouco antes de continuar o processamento.\\nNível total de tempo --> {consumo_time}')\n",
    "                    sleep(1200)\n",
    "            except:\n",
    "                print(f'Tabela {param.nm_Item_Origem} não possui header de consumo!')\n",
    "\n",
    "            # Verifica se a primeira requisição teve erro\n",
    "            if req.status_code != 200:\n",
    "                print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na primeira requisição do ID {id}!\")\n",
    "            else: # Caso a response da requisição seja 200 (sucesso)\n",
    "                # Armazena o conteúdo da API na variável data em formato JSON\n",
    "                data = req.json()\n",
    "                \n",
    "                end_point_page = None\n",
    "                data_keys = list(data.keys())\n",
    "\n",
    "                # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "                if param.nm_Definition is not None:\n",
    "                    campo_data = param.nm_Definition.split(',')\n",
    "                    if len(campo_data) == 2:\n",
    "                        data_list.extend(data[campo_data[0]][campo_data[1]])\n",
    "                    else:\n",
    "                        data_list.extend(data[campo_data[0]])\n",
    "                else:\n",
    "                    data_list.extend(data[param.nm_Item_Origem])\n",
    "\n",
    "                if 'paging' in data_keys:\n",
    "                    if 'next' in data['paging'].keys():\n",
    "                        next = data['paging']['next']\n",
    "                        end_point_page = next\n",
    "                    else:\n",
    "                        next = None\n",
    "\n",
    "                print(f'Tabela {param.nm_Item_Origem} ---> ID: {id} | {act_id} <--- ACCOUNT ID teve {pagina} páginas adicionadas com êxito!')\n",
    "\n",
    "            while next is not None:\n",
    "                req = requests.get(end_point_page, params=params)\n",
    "                data = req.json()\n",
    "                pagina += 1\n",
    "                consumo_cpu = dict(req.headers)\n",
    "\n",
    "                try:\n",
    "                    cpu = consumo['x-business-use-case-usage']\n",
    "                    cpu = json.loads(cpu)\n",
    "                    consumo_cpu = int(cpu[str(act_id)][0]['total_cputime'])\n",
    "                    consumo_time = int(cpu[str(act_id)][0]['total_time'])\n",
    "                                        \n",
    "                    if consumo_cpu > 80:\n",
    "                        print(f'Consumo excessivo de CPU da origem! Esperando um pouco antes de continuar o processamento.\\nNível da CPU ---> {consumo_cpu}')\n",
    "                        sleep(1200)\n",
    "                    elif consumo_time > 80:\n",
    "                        print(f'Consumo excessivo de tempo da origem! Esperando um pouco antes de continuar o processamento.\\nNível total de tempo --> {consumo_time}')\n",
    "                        sleep(1200)\n",
    "                except:\n",
    "                    print(f'Tabela {param.nm_Item_Origem} não possui header de consumo!')\n",
    "                \n",
    "                # Verifica se a primeira requisição teve erro\n",
    "                if req.status_code != 200:\n",
    "                    print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na página {pagina}!\")\n",
    "                    pagina -= 1\n",
    "                    print(f'{req.status_code}->{req.content}')\n",
    "                    sleep(300)\n",
    "                # Caso a response da requisição seja 200 (sucesso) \n",
    "                else:\n",
    "                    sleep(error_sleep)\n",
    "                    # Armazena o conteúdo da API na variável data em formato JSON\n",
    "                    data = req.json()\n",
    "                    end_point_page = None\n",
    "\n",
    "                    data_keys = list(data.keys())\n",
    "\n",
    "                    # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "                    if param.nm_Definition is not None:\n",
    "                        campo_data = param.nm_Definition.split(',')\n",
    "                        if len(campo_data) == 2:\n",
    "                            data_list.extend(data[campo_data[0]][campo_data[1]])\n",
    "                        else:\n",
    "                            data_list.extend(data[campo_data[0]])\n",
    "                    else:\n",
    "                        data_list.extend(data[param.nm_Item_Origem])\n",
    "\n",
    "                    if 'paging' in data_keys:\n",
    "                        if 'next' in data['paging'].keys():\n",
    "                            next = data['paging']['next']\n",
    "                            end_point_page = next\n",
    "                        else:\n",
    "                            next = None\n",
    "                    else:\n",
    "                        next = None\n",
    "                    \n",
    "                    print(f'Tabela {param.nm_Item_Origem} ---> ID: {id} | {act_id} <--- ACCOUNT ID teve {pagina} páginas adicionadas com êxito!')\n",
    "\n",
    "            for data in data_list:\n",
    "                data_format.append(data)\n",
    "\n",
    "        fim = time.time()\n",
    "        \n",
    "        # Subtrai a variável \"início\" pela \"fim\" para obter o tempo total de execução da tabela e armazena na váriavel \"tempo_exec\"\n",
    "        tempo_exec = fim - inicio\n",
    "\n",
    "        print(\"\\n\"+f\"A tabela {param.nm_Item_Origem} teve {pagina} páginas carregadas com êxito!\")\n",
    "        print(f\"Tamanho total de registros {len(data_format)}\")\n",
    "        print(f\"O tempo de execução da tabela {param.nm_Item_Origem} foi {tempo_exec}\")\n",
    "\n",
    "        # PROCESSO DE CARGA DOS DADOS NO BLOB\n",
    "        fn_SaveJson(data_format, dir_landing, str(param.ds_Nome_Arquivo_Landing).lower())\n",
    "\n",
    "        fn_AtualizaUltimoIncremento_API(param.id_Parametro_Carga_API, dtIngestao)\n",
    "    except Exception as error:\n",
    "        msg = f\"Erro na tabela: {param.ds_Nome_Arquivo_Landing} ---> ID: {id}. ERROR: {error}\"\n",
    "        print(msg)\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbcd7731-30d6-4e84-85bc-9a547f4c1710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_tables(param, baseUrl, dt_inicio_unix, dtIngestao, location_landing):\n",
    "    try:\n",
    "        format_timestamp = '%Y-%m-%d %H:%M:%S.%f'\n",
    "        dtIngestao = str(dtIngestao)\n",
    "        if param.vl_Ultimo_Incremento is not None:\n",
    "            vlUltimoIncremento = datetime.strptime(param.vl_Ultimo_Incremento, format_timestamp)\n",
    "            vlUltimoIncremento = int(mktime(vlUltimoIncremento.timetuple()))\n",
    "        else:\n",
    "            vlUltimoIncremento = dt_inicio_unix\n",
    "\n",
    "        vlUltimoIncremento = str(vlUltimoIncremento)\n",
    "\n",
    "        if param.vl_Schedule_Carga != 0:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}/{dtIngestao[11:13]}\".lower()\n",
    "        else:\n",
    "            dir_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/{param.ds_Nome_Arquivo_Landing}/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "            alter_landing = f\"{location_landing}/{param.ds_Diretorio_Landing}/@alter_table/{dtIngestao[0:4]}/{dtIngestao[5:7]}/{dtIngestao[8:10]}\".lower()\n",
    "\n",
    "        inicio = time.time()\n",
    "\n",
    "        data_format = []\n",
    "        pagina = 1\n",
    "\n",
    "        for act_id in param.nm_Campo_Fields.split(','):\n",
    "            # Declarando as variáveis\n",
    "            next = None\n",
    "            limite = 200\n",
    "            data_list = []\n",
    "\n",
    "            print(f\"Tabela {param.ds_Nome_Arquivo_Landing}! Começando carga...\")\n",
    "\n",
    "            # Pega o endpoint que utilizará no request\n",
    "            # Verificação para casos especiais que salvam a última URL que processou\n",
    "            if param.ds_Custom_Field is None:\n",
    "                end_point_page = f\"{baseUrl}{param.ds_Url.replace('@accountId', f'act_{act_id}').replace('@dtInicio', since).replace('@dtFim', until)}\"\n",
    "            else:\n",
    "                # end_point_page = baseUrl+param.ds_Custom_Field\n",
    "                end_point_page = f\"{baseUrl}{param.ds_Url.replace('@accountId', f'act_{act_id}').replace('@dtInicio', since).replace('@dtFim', until)}\"\n",
    "            \n",
    "            req = requests.get(end_point_page, params=params)\n",
    "\n",
    "            # Verifica se a primeira requisição teve erro\n",
    "            if req.status_code != 200:\n",
    "                print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na primeira requisição!\")\n",
    "                print(req.content)\n",
    "                sleep(300)\n",
    "            # Caso a response da requisição seja 200 (sucesso) \n",
    "            else:\n",
    "                # Armazena o conteúdo da API na variável data em formato JSON\n",
    "                data = req.json()\n",
    "                consumo_cpu = dict(req.headers)\n",
    "\n",
    "                try:\n",
    "                    res = consumo_cpu['x-business-use-case-usage']\n",
    "                    res = json.loads(res)\n",
    "                    consumo_cpu = int(res[act_id][0]['total_cputime'])\n",
    "                    \n",
    "                    if consumo_cpu > 80:\n",
    "                        print(f'Consumo excessivo de CPU da origem! Esperando um pouco antes de continuar o processamento.\\nNível da CPU ---> {consumo_cpu}')\n",
    "                        sleep(600)\n",
    "                except:\n",
    "                    print(f'Tabela {param.nm_Item_Origem} não possui header de consumo!')\n",
    "\n",
    "                end_point_page = None\n",
    "                data_keys = list(data.keys())\n",
    "\n",
    "                # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "                if param.nm_Definition is not None:\n",
    "                    campo_data = param.nm_Definition.split(',')\n",
    "                    if len(campo_data) == 2:\n",
    "                        data_list.extend(data[campo_data[0]][campo_data[1]])\n",
    "                    else:\n",
    "                        data_list.extend(data[campo_data[0]])\n",
    "                else:\n",
    "                    data_list.extend(data[param.nm_Item_Origem])\n",
    "       \n",
    "                if 'paging' in data_keys:\n",
    "                    if 'next' in data['paging'].keys():\n",
    "                        next = data['paging']['next']\n",
    "                        end_point_page = next\n",
    "                    else:\n",
    "                        next = None\n",
    "\n",
    "            while next is not None:\n",
    "                print(f'Começando carga página {pagina+1} da tabela {param.nm_Item_Origem}')\n",
    "                req = requests.get(end_point_page, params=params)\n",
    "                data = req.json()\n",
    "                pagina += 1\n",
    "                \n",
    "                # Verifica se há erro na tabela Ads, caso sim, carrega as páginas já processadas\n",
    "                if param.nm_Item_Origem == 'ads' and req.status_code == 400:\n",
    "                    erro = req.content\n",
    "                    print(\"\\n\"+f'Tabela {param.ds_Nome_Arquivo_Landing} ---> {id}, com erro: {erro}. Concluindo carga com dados carregados!')\n",
    "                    break\n",
    "                elif param.nm_Item_Origem == 'ad_creatives' and req.status_code != 200:\n",
    "                    print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na página {pagina}! Alterando os limites!\")\n",
    "                    pagina -= 1\n",
    "                    print(f'{req.status_code}->{req.content}')\n",
    "                    end_point_page = end_point_page.replace(f'limit={limite}', f'limit={limite-50}')\n",
    "                    limite = limite - 50\n",
    "                    sleep(60)\n",
    "                # Verifica se a primeira requisição teve erro\n",
    "                elif req.status_code != 200:\n",
    "                    print(\"\\n\"+f\"Tabela {param.ds_Nome_Arquivo_Landing} com erro na página {pagina}!\")\n",
    "                    pagina -= 1\n",
    "                    print(f'{req.status_code}->{req.content}')\n",
    "                    sleep(300)\n",
    "\n",
    "                # Caso a response da requisição seja 200 (sucesso) \n",
    "                else:\n",
    "                    # Armazena o conteúdo da API na variável data em formato JSON\n",
    "                    data = req.json()\n",
    "                    consumo_cpu = dict(req.headers)\n",
    "\n",
    "                    try:\n",
    "                        res = consumo_cpu['x-business-use-case-usage']\n",
    "                        res = json.loads(res)\n",
    "                        consumo_cpu = int(res[act_id][0]['total_cputime'])\n",
    "                        \n",
    "                        if consumo_cpu > 80:\n",
    "                            print(f'Consumo excessivo de CPU da origem! Esperando um pouco antes de continuar o processamento.\\nNível da CPU ---> {consumo_cpu}')\n",
    "                            sleep(600)\n",
    "                    except:\n",
    "                        print(f'Tabela {param.nm_Item_Origem} não possui header de consumo!')\n",
    "\n",
    "                    end_point_page = None\n",
    "\n",
    "                    data_keys = list(data.keys())\n",
    "\n",
    "                    # Algumas tabelas salvam a partir de colunas com o nome diferente da tabela\n",
    "                    if param.nm_Definition is not None:\n",
    "                        campo_data = param.nm_Definition.split(',')\n",
    "                        if len(campo_data) == 2:\n",
    "                            data_list.extend(data[campo_data[0]][campo_data[1]])\n",
    "                        else:\n",
    "                            data_list.extend(data[campo_data[0]])\n",
    "                    else:\n",
    "                        data_list.extend(data[param.nm_Item_Origem])\n",
    "\n",
    "                    if 'paging' in data_keys:\n",
    "                        if 'next' in data['paging'].keys():\n",
    "                            next = data['paging']['next']\n",
    "                            end_point_page = next\n",
    "                        else:\n",
    "                            next = None\n",
    "                    else:\n",
    "                        next = None\n",
    "\n",
    "            for data in data_list:\n",
    "                data_format.append(data)\n",
    "\n",
    "        fim = time.time()\n",
    "        \n",
    "        # Subtrai a variável \"início\" pela \"fim\" para obter o tempo total de execução da tabela e armazena na váriavel \"tempo_exec\"\n",
    "        tempo_exec = fim - inicio\n",
    "\n",
    "        print(\"\\n\"+f\"A tabela {param.nm_Item_Origem} teve {pagina} páginas carregadas com êxito!\")\n",
    "        print(f\"Tamanho total de registros {len(data_format)}\")\n",
    "        print(f\"O tempo de execução da tabela {param.nm_Item_Origem} foi {tempo_exec}\")\n",
    "\n",
    "        # PROCESSO DE CARGA DOS DADOS NO BLOB\n",
    "        fn_SaveJson(data_format, dir_landing, str(param.ds_Nome_Arquivo_Landing).lower())\n",
    "\n",
    "        fn_AtualizaUltimoIncremento_API(param.id_Parametro_Carga_API, dtIngestao)\n",
    "\n",
    "        if param.nm_Item_Origem in ['ad_campaigns']:\n",
    "            list_ids = []\n",
    "            list_act_ids = []\n",
    "            subtables_campaigns = ['ad_insights','ad_insights_age_and_gender','ad_insights_country','ad_insights_dma','ad_insights_hourly_stats_by_audience_timezone','ad_insights_platform_device','ad_insights_region']\n",
    "\n",
    "            data_param_id = df_param_id.filter(col('nm_Item_Origem').isin(subtables_campaigns)).collect()\n",
    "          \n",
    "            for data in data_format:\n",
    "                list_ids.append(data['id'])\n",
    "                list_act_ids.append(data['account_id'])\n",
    "\n",
    "            for row in data_param_id:\n",
    "                # Executa a função fn_StreamFromFolder_csv em uma thread do ThreadPoolExecutor\n",
    "                subtask = executor.submit(get_tables_from_ids, *(row, baseUrl, dt_inicio_unix, dt_fim, location_landing, list_ids, list_act_ids))\n",
    "                # Adiciona a tarefa à lista de tarefas\n",
    "                subtasks.append(subtask)\n",
    "\n",
    "    except Exception as error:\n",
    "        msg = f\"Erro na tabela: {param.ds_Nome_Arquivo_Landing}. ERROR: {error}\"\n",
    "        print(msg)\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b4c9dd2-73ab-4c84-88ad-3015657efa0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cria uma lista para armazenar todas as tarefas\n",
    "tasks = []\n",
    "subtasks = []\n",
    "\n",
    "# Cria uma instância do ThreadPoolExecutor com threads definidas (max_workers)\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=threads)\n",
    "\n",
    "# Percorre todas as pastas do diretório de origem\n",
    "for row in data_param:\n",
    "    # Executa a função fn_StreamFromFolder_csv em uma thread do ThreadPoolExecutor\n",
    "    task = executor.submit(get_tables, *(row, baseUrl, dt_inicio_unix, dt_fim, location_landing))\n",
    "    # Adiciona a tarefa à lista de tarefas\n",
    "    tasks.append(task)\n",
    "\n",
    "# Aguarda a conclusão de todas as tarefas\n",
    "_ = concurrent.futures.wait(tasks, return_when='ALL_COMPLETED')\n",
    "\n",
    "executor.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9bdcdc2-f2c5-40c2-bff1-7231efef8cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tarefas = tasks + subtasks\n",
    "\n",
    "for task in tarefas:\n",
    "    try:\n",
    "        print(task.result(),'\\n')\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Process_Ingestao_VR_Gente_Facebook_Ads",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
